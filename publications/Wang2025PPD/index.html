
<html>
    <head>
        <title> Preserving Photographic Defocus in Stylized Image Synthesis </title>
        <meta HTTP-EQUIV="Content-Type" CONTENT="text/html;charset=UTF-8">
        <link rel="icon" href="../../images/icon.png">
        <link href="../../css/style.css" rel="stylesheet">
    </head>
    
    <body>
        <div style="margin-left:auto; margin-right:auto; width:800">
        <h3 class="my-3">Preserving Photographic Defocus in Stylized Image Synthesis</h3>
        
        Hong-Yi Wang &nbsp;&nbsp;&nbsp;
        <a href="https://kevincosner.github.io/">Yu-Ting Wu</a> &nbsp;&nbsp;&nbsp;
        
        <br>
        <a href="http://www.ntu.edu.tw/">National Taipei University</a>
        <br>
        <br>
    
        <div><a href="images/teaser.png"><img class="img-fluid rounded" src="images/teaser.png" width=800></a></div>
        <div>
            <p>
            By integrating our method with existing style transfer techniques, including attention-aware [Zhang et al. 2024], depth-aware [Ioannou and Maddock 2022], aesthetic-aware [Hu et al. 2020], and parameterized brushstrokes-based [Kotovenko et al. 2021] methods, we effectively preserve the original defocus effects intended by the photographer. The incorporation of defocus into the stylized outputs further enhances structural coherence, allowing the background to fade naturally and improving visual separation and emphasis on focal regions. For each method, the corresponding content and style images are presented in the first column.
            </p>
        </div>
    
        <hr class="style-two">
        <p>
        <font size=+1><b>Abstract</b></font><br>
        While style transfer has been extensively studied, most existing approaches fail to account for the defocus effects inherent in content images, thereby compromising the photographerâ€™s intended focus cues. To overcome this shortcoming, we introduce an optimization-based post-processing framework that restores defocus characteristics to stylized images, regardless of the style transfer technique used. Our method initiates by estimating a blur map through a data-driven model that predicts pixel-level blur magnitudes. This blur map subsequently guides a layer-based defocus rendering framework, which effectively simulates depth-of-field (DoF) effects using a Gaussian filter bank. To map the blur values to appropriate kernel sizes in the filter bank, we introduce a neural network that determines the optimal maximum filter size, ensuring both content integrity and stylistic fidelity. Experimental results, both quantitative and qualitative, show that our method significantly improves stylized images by preserving the original depth cues and defocus details.
        </p>
        <hr class="style-two">
    
        <p>
        <font size=+1><b>Publication</b></font><br>
        Hong-Yi Wang, Yu-Ting Wu. <br>
        Preserving Photographic Defocus in Stylized Image Synthesis. <br>
        Computer Graphics Forum, to appear. <a target="_blank" href="">BibTeX (coming soon)</a><br>
        <a target="_blank" href="">CGF paper (coming soon)</a><br>
        <a target="_blank" href="">Digital library (coming soon)</a>
        </p>
        <hr class="style-two">
    
        <br><br>
        <hr class="style-two">
        <p>Last Update: June 2025</p>
    
    </body>
</html>
    