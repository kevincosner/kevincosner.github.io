<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>Yu-Ting Wu's Research Homepage</title>
        <link rel="icon" href="images/icon.png">
        <link href="css/style.css" rel="stylesheet">
        <link rel="stylesheet" href="css/lightbox2/dist/css/lightbox.min.css">
    </head>

    <body>
        <!-- Navigation Bar -->
        <nav class="navbar navbar-dark navbar-expand-lg fixed-top" style="background-color: #17a2b8;">
            <div class="container">
                <table>
                    <tr>
                        <td><a class="nav-link" href="#" style="color: #fff;">Home<span class="sr-only">(current)</span></a></td>
                        <td><a class="nav-link" href="#news" style="color: #fff;">News</a></td>
                        <td><a class="nav-link" href="#bio" style="color: #fff;">Biography</a></td>
                        <td><a class="nav-link" href="#experience" style="color: #fff;">Experience</a></td>
                        <!--<td><a class="nav-link" href="#recent_projects" style="color: #fff;">Projects</a></td>-->
                        <td><a class="nav-link" href="#publications" style="color: #fff;">Publications</a></td>
                        <!--<td><a class="nav-link" href="#misc" style="color: #fff;">Misc</a></td>-->
                    </tr>
                </table>
            </div>
        </nav>

        <!-- Page Content -->
        <div class="container mt-4">
            <!-- Portfolio Item Row -->
            <div class="row">
                <div class="col-md-4">
                    <img class="img-fluid rounded" src="images/kevin8.jpg" alt="" width="185px">
                </div>
  
                <div class="col-md-13">
                    <h3 class="my-3">Yu-Ting Wu <span style="font-family: sans-serif">(吳昱霆)</span> <small><br>Postdoctoral Researcher</small> </h3>
  
                    <p>
                        <i class="far fa-building"></i> Communication and Multimedia Laboratory (CMLab)<br>
                        <i class="far fa-building" style="color:transparent"></i> National Taiwan University<br>
                    </p>
                    <p>
                        <i class="far fa-envelope"></i> kevincosnerwu [at] gmail.com
                    </p>
                    <p>
                        <i class="far fa-folder-open"></i>
                        <a href="CV_webpage_acdemic_may_2021.pdf" target="_blank">Curriculum Vitae</a>
                    </p>
                </div>
            </div>

            <a class="anchor" name="news"></a>
            <hr class="style-two">
            <h3 class="my-3">Recent News</h3>
            <table class="table">
                <thead class="thead-light">
                    <tr>
                        <th scope="col">Time</th>
                        <th scope="col">Event</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>May 2021</a></td>
                        <!--<td>Our paper, "Multi-Resolution Shared Representative Filtering for Real-Time Depth Completion", is accepted by High-Performance Graphics 2021</td>-->
                        <td>Our paper, "Multi-Resolution Shared Representative Filtering for Real-Time Depth Completion", is accepted</td>
                    </tr>
                    <tr>
                        <td>Mar. 2021</a></td>
                        <td>Served as a reviewer for APMAR 2021</td>
                    </tr>
                    <tr>
                        <td>Dec. 2020</td>
                        <td>Our paper, "ClipFlip: Multi-view Clipart Design", is accepted to Computer Graphics Forum</td>
                    </tr>
                    <tr>
                        <td>Dec. 2020</a></td>
                        <td>Served as a reviewer for CVPR 2021</td>
                    </tr>
                    <tr>
                        <td>Sep. 2020</a></td>
                        <td>My son was born!</td>
                    </tr>
                    <tr>
                        <td>May 2020</a></td>
                        <td>Served as a reviewer for ECCV 2020</td>
                    </tr>
                    <tr>
                        <td>Feb. 2020</a></td>
                        <td>Joined Communication and Multimedia Laboratory (CMLab) of National Taiwan University as a postdoctoral researcher</td>
                    </tr>
                </tbody>
            </table>

            <a class="anchor" name="bio"></a>
            <hr class="style-two">
            <h3 class="my-3">Biography</h3>
            <p>I am currently a postdoctoral researcher at <a href="https://www.cmlab.csie.ntu.edu.tw/new_cml_website/index.php" target="_blank">Communication and Multimedia Laboratory (CMLab)</a>, 
				 National Taiwan University. I recieved my bachelor's degree and master's degree in National Chiao Tung University (NCTU) in 2007 and 2009, respectively. After that, 
				 I continued my research with Prof. <a href="https://www.csie.ntu.edu.tw/~cyy/" target="_blank">Yung-Yu Chuang</a> in National Taiwan University (NTU) and defended my 
                 Ph.D. degree in 2014. During the period 2014 to 2019, I worked as a software engineer at HTC and Toppano, mainly in charge of developing computer graphics and 
                 computational photography techniques for VR and MR applications. My research interests include computer graphics, computational photography, computer vision, AR/MR/VR 
                 , and machine learning.
            </p>

            <a class="anchor" name="experience"></a>
            <hr class="style-two">
            <h3 class="my-3">Experience</h3>
            <p><b>Postdoctoral Researcher - National Taiwan University</b>, Feb. 2020 - Present</p>            
            <p><b>Senior Algorithm Developer - Toppano Inc. (a startup company)</b>, May 2018 - Jan. 2020</p>
            <p><b>Principal Engineer - HTC Inc.</b>, Sep. 2014 - Apr. 2018</p>
            <p><b>Summer Intern - Digimax Inc.</b>, Jul. 2011 - Sep. 2011</p>
            <p>Teaching Assistant (Digital Image Synthesis) - National Taiwan University, Fall 2009 - 2013</p>
            <p>Teaching Assistant (Computer Graphics) - National Chiao Tung University, Fall 2008</p>
            
            <a class="anchor" name="publications"></a>
            <hr class="style-two">
            <h3 class="my-3">Publications</h3>
            <class="my-4">Categorized by: &nbsp</class> 
            Computer Graphics <b class="label_cg">CG</b> &nbsp
            Computer Vision <b class="label_cv">CV</b> &nbsp
            Image Processing <b class="label_ip">IP</b> &nbsp
            Machine Learning <b class="label_learning">ML</b> &nbsp
            <br><br>

            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <a class="example-image-link" href="publications/Wu2021MSR/thumb.png" data-lightbox="example-Wu2021MSR" 
                       data-title="Multi-Resolution Shared Representative Filtering for Real-Time Depth Completion">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/Wu2021MSR/thumb.png" alt="Multi-Resolution Shared Representative Filtering"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>Multi-Resolution Shared Representative Filtering for Real-Time Depth Completion</b> &nbsp <b class="label_cg">CG</b> <b class="label_cv">CV</b> <b class="label_ip">IP</b> <br>
                    <u>Yu-Ting Wu</u>, Tzu-Mao Li, I-Chao Shen, Hong-Shiang Lin, Yung-Yu Chuang<br>
                    <!--<i>Accepted to High-Performance Graphics 2021</i><br>-->
                    <i>To appear</i><br>
                    <span>(a real-time depth completion algorithm which can effectively handle large missing regions of depth maps)</span><br>
                    <br>
                    <!--
                    <span style="background-color: #ffffff;color: #1577A4;"><a data-toggle="collapse" data-target="#wu_2021_msr_abstract" aria-expanded="false">show / hide abstract ...</a></span>
                    </p>
					<div class="collapse" id="wu_2021_msr_abstract">
                        <p>&nbsp&nbsp&nbsp&nbsp We present shared representative filtering for real-time high-resolution depth completion with RGB-D sensors. Conventional
                            filtering-based methods face a dilemma when the missing regions of the depth map are large. When the filter window is small, the filter fails 
                            to include enough samples. On the other hand, when the window is large, the method could oversmooth depth boundaries due to the error introduced 
                            by the extra samples. Our method adapts the filter kernels to the shape of the missing regions to collect a sufficient number of samples while 
                            avoiding oversmoothing. We collect depth samples by searching for a small set of similar pixels, which we call the representatives, using an 
                            efficient line search algorithm. We then combine the representatives using a joint bilateral weight. Experiments show that our method can filter 
                            a high-resolution depth map within a few milliseconds while outperforming previous filtering-based methods on both real-world and synthetic data 
                            in terms of both efficiency and accuracy, especially when dealing with large missing regions in depth maps.
                        </p>
                    </div>
                    -->
                    <a href="" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Coming Soon</a>    
                    <!--
                    <a href="publications/Wu2021MSR/index.html" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Project Page</a>
                    <a href="" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Paper (Coming Soon)</a>
                    <a href="" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Citation (Coming Soon)</a>
                    <a href="" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Digital Library (Coming Soon)</a>
                    -->
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <!--<img src="publications/Shen2020CMC/thumb.png" class="img-thumbnail center-block" style="max-width:100%">-->
                    <a class="example-image-link" href="publications/Shen2020CMC/thumb.png" data-lightbox="example-Shen2020CMC" 
                       data-title="Multi-view Clipart Design">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/Shen2020CMC/thumb.png" alt="Multi-view Clipart Design"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>ClipFlip: Multi-view Clipart Design</b> &nbsp <b class="label_cg">CG</b> <b class="label_cv">CV</b> <b class="label_learning">ML</b> <br>
                    I-Chao Shen, Kuan-Hung Liu, Li-Wen Su, <u>Yu-Ting Wu</u>, Bing-Yu Chen<br>
                    <i>Computer Graphics Forum, February 2021</i><br>
                    <span>(an assistive system for clipart design by providing visual scaffolds from the unseen view points)</span><br>
                    <span style="background-color: #ffffff;color: #1577A4;"><a data-toggle="collapse" data-target="#shen_2020_cmc_abstract" aria-expanded="false">show / hide abstract ...</a></span>
                    </p>
					<div class="collapse" id="shen_2020_cmc_abstract">
                        <p>&nbsp&nbsp&nbsp&nbsp We present an assistive system for clipart design by providing visual scaffolds from the unseen viewpoints. Inspired by the artists’
                            creation process, our system constructs the visual scaffold by first synthesizing the reference 3D shape of the input clipart and rendering it from the 
                            desired viewpoint. The critical challenge of constructing this visual scaffold is to generate a reference 3D shape that matches the user’s expectations 
                            in terms of object sizing and positioning while preserving the geometric style of the input clipart. To address this challenge, we propose a user-assisted 
                            curve extrusion method to obtain the reference 3D shape. We render the synthesized reference 3D shape with a consistent style into the visual scaffold. 
                            By following the generated visual scaffold, the users can efficiently design clipart with their desired viewpoints. The user study conducted by an intuitive
                            user interface and our generated visual scaffold suggests that our system is especially useful for estimating the ratio and scale between object parts and 
                            can save on average 57% of drawing time.
                        </p>
                    </div>
                    <a href="publications/Shen2020CMC/index.html" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Project Page</a>
                    <a href="publications/Shen2020CMC/paper.pdf" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Paper</a>
                    <a href="publications/Shen2020CMC/citation.bib" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Citation</a>
                    <a href="https://arxiv.org/abs/2008.12933" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">arXiv version</a>
                    <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14190" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Digital Library</a>
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <!--<img src="publications/Wu2015DMS/thumb.jpg" class="img-thumbnail center-block" style="max-width:100%">-->
                    <a class="example-image-link" href="publications/Wu2015DMS/dms.gif" data-lightbox="example-Wu2015DMS" 
                       data-title="Dual-Matrix Sampling for Scalable Translucent Material Rendering, IEEE Transactions on Visualization and Computer Graphics, 2015">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/Wu2015DMS/thumb.jpg" alt="Dual-Matrix Sampling"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>Dual-Matrix Sampling for Scalable Translucent Material Rendering</b> &nbsp <b class="label_cg">CG</b> <br>
                    <u>Yu-Ting Wu</u>, Tzu-Mao Li, Yu-Hsun Lin, Yung-Yu Chuang<br>
                    <i>IEEE Transactions on Visualization and Computer Graphics, March 2015</i><br>
                    <span>(a scalable algorithm for rendering plenty of translucent objects under complex illumination)</span><br>
                    <span style="background-color: #ffffff;color: #1577A4;"><a data-toggle="collapse" data-target="#wu_2015_dms_abstract" aria-expanded="false">show / hide abstract ...</a></span>
                    </p>
					<div class="collapse" id="wu_2015_dms_abstract">
                        <p>&nbsp&nbsp&nbsp&nbsp This paper introduces a scalable algorithm for rendering translucent materials with complex lighting. We represent the light transport 
                           with a diffusion approximation by a dual-matrix representation with the Light-to-Surface and Surface-to-Camera matrices. By exploiting the structures within 
                           the matrices, the proposed method can locate surface samples with little contribution by using only subsampled matrices and avoid wasting computation on these 
                           samples. The decoupled estimation of irradiance and diffuse BSSRDFs also allows us to have a tight error bound, making the adaptive diffusion approximation 
                           more efficient and accurate. Experiments show that our method outperforms previous methods for translucent material rendering, especially in large scenes with 
						   massive translucent surfaces shaded by complex illumination.
                        </p>
                    </div>
                    <a href="publications/Wu2015DMS/index.html" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Project Page</a>
                    <a href="publications/Wu2015DMS/paper.pdf" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Paper</a>
                    <a href="publications/Wu2015DMS/citation.bib" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Citation</a>
                    <a href="https://ieeexplore.ieee.org/document/6994841" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Digital Library</a>
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <!--<img src="publications/Wu2013VC/thumb.jpg" class="img-thumbnail center-block" style="max-width:100%">-->
                    <a class="example-image-link" href="publications/Wu2013VC/vc2013.gif" data-lightbox="example-Wu2013VC" 
                       data-title="VisibilityCluster: Average Directional Visibility for Many-Light Rendering, IEEE Transactions on Visualization and Computer Graphics, 2013">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/Wu2013VC/thumb.jpg" alt="VisibilityCluster"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>VisibilityCluster: Average Directional Visibility for Many-Light Rendering</b> &nbsp <b class="label_cg">CG</b> <br>
                    <u>Yu-Ting Wu</u>, Yung-Yu Chuang<br>
                    <i>IEEE Transactions on Visualization and Computer Graphics, September 2013</i><br>
                    <span>(a method for efficient computation and compact representation of the visibility function for many-light rendering)</span><br>
                    <span style="background-color: #ffffff;color: #1577A4;"><a data-toggle="collapse" data-target="#wu_2013_vc_abstract" aria-expanded="false">show / hide abstract ...</a></span>
                    </p>
					<div class="collapse" id="wu_2013_vc_abstract">
                        <p>&nbsp&nbsp&nbsp&nbsp This paper proposes the VisibilityCluster algorithm for efficient visibility approximation and representation in many-light rendering. 
                           By carefully clustering lights and shading points, we can construct a visibility matrix that exhibits good local structures due to visibility coherence of 
                           nearby lights and shading points. Average visibility can be efficiently estimated by exploiting the sparse structure of the matrix and shooting only few 
                           shadow rays between clusters. Moreover, we can use the estimated average visibility as a quality measure for visibility estimation, enabling us to locally 
                           refine VisibilityClusters with large visibility variance for improving accuracy. We demonstrate that, with the proposed method, visibility can be incorporated 
                           into importance sampling at a reasonable cost for the manylight problem, significantly reducing variance in Monte Carlo rendering. In addition, the proposed 
						   method can be used to increase realism of local shading by adding directional occlusion effects. Experiments show that the proposed technique outperforms 
						   state-ofthe-art importance sampling algorithms, and successfully enhances the preview quality for lighting design.
                        </p>
                    </div>
                    <a href="publications/Wu2013VC/index.html" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Project Page</a>
                    <a href="publications/Wu2013VC/paper.pdf" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Paper</a>
                    <a href="publications/Wu2013VC/citation.bib" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Citation</a>
                    <a href="https://ieeexplore.ieee.org/document/6464264" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Digital Library</a>
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <!--<img src="publications/Li2012SBO/thumb.png" class="img-thumbnail center-block" style="max-width:100%">-->
                    <a class="example-image-link" href="publications/Li2012SBO/thumb.png" data-lightbox="example-Li2012SBO" 
                       data-title="SURE-based Optimization for Adaptive Sampling and Reconstruction, ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2012)">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/Li2012SBO/thumb.png" alt="SURE-based Optimization"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>SURE-based Optimization for Adaptive Sampling and Reconstruction</b> &nbsp <b class="label_cg">CG</b> <br>
                    Tzu-Mao Li, <u>Yu-Ting Wu</u>, Yung-Yu Chuang<br>
                    <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2012), </i>
                    <span style="background-color: #ffffff;color: #1577A4;"><i>selected as a highlight paper by the program chair</i></span><br>
					<span>(an adaptive sampling and denoising method for Monte Carlo rendering using Stein's unbiased risk estimator)</span><br>
                    <span style="background-color: #ffffff;color: #1577A4;"><a data-toggle="collapse" data-target="#li_2012_sbo_abstract" aria-expanded="false">show / hide abstract ...</a></span>
                    </p>
					<div class="collapse" id="li_2012_sbo_abstract">
                        <p>&nbsp&nbsp&nbsp&nbsp We apply Stein’s Unbiased Risk Estimator (SURE) to adaptive sampling and reconstruction to reduce noise in Monte Carlo rendering. 
                           SURE is a general unbiased estimator for mean squared error (MSE) in statistics. With SURE, we are able to estimate error for an arbitrary reconstruction 
                           kernel, enabling us to use more effective kernels rather than being restricted to the symmetric ones used in previous work. It also allows us to allocate 
                           more samples to areas with higher estimated MSE. Adaptive sampling and reconstruction can therefore be processed within an optimization framework. We also 
                           propose an efficient and memory-friendly approach to reduce the impact of noisy geometry features where there is depth of field or motion blur. Experiments 
                           show that our method produces images with less noise and crisper details than previous methods.
                        </p>
                    </div>
                    <a href="publications/Li2012SBO/index.html" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Project Page</a>
                    <a href="publications/Li2012SBO/paper.pdf" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Paper</a>
                    <a href="publications/Li2012SBO/slides.pptx" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Slides</a>
					<a href="publications/Li2012SBO/citation.bib" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Citation</a>
                    <a href="https://dl.acm.org/doi/10.1145/2366145.2366213" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Digital Library</a>
                </div>
            </div>
            
            <a class="anchor" name="publications"></a>
            <hr class="style-two">
            <h3 class="my-3">Short Papers and Posters</h3>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <!--<img src="publications/Wu2012VC/thumb.jpg" class="img-thumbnail center-block" style="max-width:100%">-->
                    <a class="example-image-link" href="publications/Wu2012VC/vc2012.gif" data-lightbox="example-Wu2012VC" 
                       data-title="VisibilityChunk: Average Directional Visibility for Importance Sampling, ACM SIGGRAPH Asia 2012 Poster">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/Wu2012VC/thumb.jpg" alt="VisibilityChunk"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>VisibilityChuck: Average Directional Visibility for Importance Sampling</b> &nbsp <b class="label_cg">CG</b> <br>
                    <u>Yu-Ting Wu</u>, Yung-Yu Chuang<br>
                    <i>ACM SIGGRAPH Asia 2012 Poster, </i>
					<span style="background-color: #ffffff;color: #1577A4;"><i>selected as a highlight poster by the program chair</i></span><br>
					<span>(an early version of our VisibilityCluster paper)</span><br>
                    </p>
                    <!--<br>-->
                    <a href="publications/Wu2012VC/paper.pdf" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Paper</a>
                    <a href="https://dl.acm.org/doi/10.1145/2407156.2407205" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Digital Library</a>
                </div>
            </div>

            <a class="anchor" name="dissertation"></a>
            <hr class="style-two">
            <h3 class="my-3">Ph.D. Dissertation</h3>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <!--<img src="publications/dissertation/thumb.jpg" class="img-thumbnail center-block" style="max-width:100%">-->
                    <a class="example-image-link" href="publications/dissertation/thumb.jpg" data-lightbox="example-dissertation" 
                       data-title="Sampling and Reconstruction Techniques for Efficient Monte Carlo Rendering, Ph.D. Dissertation, National Taiwan University 2014">
                       <img class="img-thumbnail center-block" style="max-width:100%" src="publications/dissertation/thumb.jpg" alt="Ph.D. Dissertation"/>
                    </a>
                </div>
                <div class="col-md-9">
                    <p><b>Sampling and Reconstruction Techniques for Efficient Monte Carlo Rendering</b> &nbsp <b class="label_cg">CG</b> <br>
                    <u>Yu-Ting Wu</u>, advised by Yung-Yu Chuang<br>
                    <i>Doctor of Philosophy in Computer Science and Information Engineering, National Taiwan University, June 2014</i></br>
                    <span>(a coherent view of my Ph.D. research with background reviews)</span><br>
                    <span style="background-color: #ffffff;color: #1577A4;"><a data-toggle="collapse" data-target="#wu_2014_dissertation_abstract" aria-expanded="false">show / hide abstract ...</a></span>
                    </p>
					<div class="collapse" id="wu_2014_dissertation_abstract">
                        <p>&nbsp&nbsp&nbsp&nbsp Two of the most important tasks that computer graphics techniques try to solve is rendering photo-realistic images and performing 
                           numerically accurate simulation. Physically-based rendering can naturally satisfy these two goals. It is usually simulated by the Monte Carlo ray tracing 
                           for handling a variety of sophisticated light transport paths in a united manner. Despite its generality and simplicity, however, Monte Carlo integration 
                           converges slowly. Rendering scenes with lots of complex geometry and realistic materials under complex illumination usually requires a large number of 
                           samples to produce a noise-free image. <br>
                           &nbsp&nbsp&nbsp&nbsp In this dissertation, we proposed three advanced sampling and reconstruction algorithms for improving the performance of Monte Carlo 
                           integration. First, realizing that in complex scenes visibility is usually the major source of noise during sampling the shading function, we developed a 
                           method called VisibilityCluster for efficiently approximating visibility function. By integrating it into importance sampling framework, we achieve 
                           superior noise reduction compared to previous approaches. Second, to reduce the computation overhead of rendering translucent materials, we proposed an 
                           algorithm, Dualmatrix sampling, to avoid evaluating unimportant surface samples which contribute little to the final image. Finally, a general adaptive 
                           sampling and reconstruction framework named SURE-based optimization is proposed to render a wide range of distributed effects, including depth of field, 
                           motion blur, and global illumination. All of the three methods achieve significant performance improvement compared to the state-of-the-art rendering 
                           algorithms.
                        </p>
                    </div>
                    </p>
                    <a href="publications/dissertation/dissertation.pdf" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Dissertation</a>
                    <a href="publications/dissertation/slides.pptx" target="_blank" class="btn btn-outline vert-offset-top-1" role="button">Slides</a>
                </div>
            </div>
            
            <a class="anchor" name="patents"></a>
            <hr class="style-two">
            <h3 class="my-3">Patents</h3>
            <p>
                <b>Electronic device, method for displaying an augmented reality scene and non-transitory computer-readable medium</b> &nbsp <b class="label_cg">CG</b> <b class="label_cv">CV</b> <br>
                Yu-Ting Wu, Ching-Yang Chen<br>
                ROC Patent No: I711966. December 01, 2020<br>
                US  Patent No: 10636200, April 28, 2020
            </p>            
            
            <p>
                <b>Virtual reality device, image processing method, and non-transitory computer-readable medium</b> &nbsp <b class="label_cg">CG</b> <br>
                Yu-Ting Wu, Chun-Wen Cheng, Ching-Yang Chen<br>
                ROC Patent No: I684163, February 01, 2020
            </p> 

            <p>
                <b>Three-dimensional modeling method and electronic apparatus thereof</b> &nbsp <b class="label_cg">CG</b> <b class="label_cv">CV</b> <br>
                Sheng-Jie Luo, Liang-Kang Huang, Yu-Ting Wu, Tung-Peng Wu<br>
                US  Patent No: 10152827, December 11, 2018
            </p>

            <!--
            <a class="anchor" name="misc"></a>
            <hr class="style-two">
            <h3 class="my-3">Misc</h3>
            -->
            

            <br><br><br>
            <hr class="style-two">
            <p>The design of this webpage is borrowed from Dr. Lingqi Yan</p>
            <p>Last Update: May 2021</p>

        </div>

        <!-- Bootstrap core JavaScript -->
        <script src="vendor/jquery/jquery.min.js"></script>
        <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
        <script src="css/lightbox2/dist/js/lightbox-plus-jquery.min.js"></script>

    </body>

</html>